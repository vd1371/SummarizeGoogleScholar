{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import time\n",
    "import random\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'DNT': '1',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "}\n",
    "\n",
    "def get_scholar_papers(scholar_url: str, sort_by: str = 'citations', num_papers: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scrape paper links from a Google Scholar profile using BeautifulSoup\n",
    "    \n",
    "    Args:\n",
    "        scholar_url (str): URL of the Google Scholar profile\n",
    "        sort_by (str): Sort criteria - 'citations' or 'year'\n",
    "        num_papers (int): Number of paper links to return\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of paper URLs\n",
    "    \"\"\"\n",
    "    time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # Add sorting parameter to URL\n",
    "    scholar_url += '&view_op=list_works'\n",
    "    if sort_by.lower() == 'year':\n",
    "        scholar_url += '&sortby=pubdate'\n",
    "\n",
    "    elif sort_by.lower() == 'citations':\n",
    "        scholar_url += '&sort=citations'\n",
    "    else:\n",
    "        raise ValueError(\"sort_by must be either 'citations' or 'year'\")\n",
    "    \n",
    "    paper_links = []\n",
    "    \n",
    "    try:\n",
    "        # Make request with headers\n",
    "        response = requests.get(scholar_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all paper links\n",
    "        articles = soup.find_all('a', class_='gsc_a_at')\n",
    "        \n",
    "        # Extract links from articles\n",
    "        for article in articles[:num_papers]:\n",
    "            if article.get('href'):\n",
    "                paper_links.append('https://scholar.google.com' + article.get('href'))\n",
    "            \n",
    "            # Add random delay between requests\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "            \n",
    "        return paper_links[:num_papers]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def get_paper_details(paper_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Scrape paper details from a Google Scholar paper URL\n",
    "    \n",
    "    Args:\n",
    "        paper_url (str): URL of the Google Scholar paper\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing paper details (title, description, year, journal)\n",
    "    \"\"\"\n",
    "    paper_details = {\n",
    "        'title': '',\n",
    "        'description': '',\n",
    "    }\n",
    "\n",
    "\n",
    "    # title xpath: //*[@id=\"gsc_oci_title\"]/a/text()\n",
    "    # Description xpath: //*[@id=\"gsc_oci_descr\"]/div/div/div\n",
    "    \n",
    "    try:\n",
    "        # Add random delay before request\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "        \n",
    "        response = requests.get(paper_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse with lxml for xpath support\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        \n",
    "        # Extract title\n",
    "        title_elem = soup.find('div', id='gsc_oci_title')\n",
    "        if title_elem:\n",
    "            paper_details['title'] = title_elem.text.strip()\n",
    "        \n",
    "        # Extract description using xpath\n",
    "        desc_elem = soup.find('div', id='gsc_oci_descr')\n",
    "        if desc_elem:\n",
    "            paper_details['description'] = desc_elem.text.strip()\n",
    "        \n",
    "        return paper_details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching paper details: {e}\")\n",
    "        return paper_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "role = \"\"\"\n",
    "You are a helpful assistant that helps with writing emails.\n",
    "In you role, you are responsible for reading the title and description of a\n",
    "a few papers and then showing interest in the author's work.\n",
    "\n",
    "you will receive the resume of the applicant and your summary and the output\n",
    "must relate the background of the applicant to the papers and show interest\n",
    "in the author's work and say it's a good fit for the author's work.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "\n",
    "here's the resume of the applicant:\n",
    "\n",
    "{resume}\n",
    "\n",
    "here's the title and abstract of the papers:\n",
    "\n",
    "{papers}\n",
    "\n",
    "Use the resume information and the abstract and title of the papers to write 50 words that will be sent to the author.\n",
    "\n",
    "in these 50 words, you must mentions the author's works and how my background is a good fit for the author's research.\n",
    "you sentences must be just related to this and not anything else.\n",
    "\n",
    "Just write these 50 words, don't write anything else.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Her:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    def write_email(self, resume: str, papers: List[dict[str, str]]) -> str:\n",
    "        \n",
    "        papers_str = \"\"\n",
    "        for paper in papers:\n",
    "            papers_str += f\"Title: {paper['title']}\\Abstract: {paper['description']}\\n\\n\"\n",
    "        \n",
    "        context = resume\n",
    "        query = prompt.format(resume=resume, papers=papers_str)\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": role},\n",
    "            {\"role\": \"user\",\n",
    "              \"content\": f\"Context: {context}\\n\\n{query}\"}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_scholar_papers(\n",
    "  \"The google scholar url of the author\",\n",
    "  sort_by=\"year\", # Citations or year\n",
    "  num_papers=1 # Number of papers to return\n",
    ")\n",
    "\n",
    "all_papers = []\n",
    "for link in results:\n",
    "    paper_info = get_paper_details(link)\n",
    "    all_papers.append(paper_info)\n",
    "\n",
    "resume = \"\"\"\n",
    "\n",
    "Put your resume here\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "her = Her()\n",
    "her.write_email(resume, all_papers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
